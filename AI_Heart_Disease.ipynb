{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3tF+4edMPcKzweWVeqZiE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asya-bamby/An-AI-model-for-heart-disease-detection/blob/main/AI_Heart_Disease.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preparatory work.**\n",
        "___________________________________________\n",
        "\n",
        "Installing all necessary libraries"
      ],
      "metadata": {
        "id": "w7va99i9KXwc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFyUVCpTINu-",
        "outputId": "2a0ada5d-5758-4ffa-f7d7-6e31d3a7121d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gdown in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown) (4.13.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown) (3.19.1)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.12/dist-packages (from gdown) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from gdown) (4.67.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (2025.8.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade gdown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hje-i48nKxvT",
        "outputId": "b0648c0a-fc7c-4702-d20d-5faa051c8239"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import uniform\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc, confusion_matrix, precision_score, recall_score\n",
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "from tqdm.autonotebook import tqdm\n",
        "pd.set_option('display.max_columns', None)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', module='pandas')\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHaK4VN7K4EJ",
        "outputId": "38cda964-35d8-4277-8fab-47e57365fca6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4112232649.py:24: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will write *frequently* used functions"
      ],
      "metadata": {
        "id": "sr8URvEeK_ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def anomaly(data):\n",
        "    \"\"\"\n",
        "    Analyzes basic statistics and anomalies in the data.\n",
        "\n",
        "    The function performs an exploratory data analysis of a dataset, displaying key statistics\n",
        "    for each column: unique values, counts, missing values, and data dimensions.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : pandas.Series or pandas.DataFrame\n",
        "        Input data for analysis. If Series is provided, it will be converted to DataFrame.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    None\n",
        "        The function prints results to the console without returning any value.\n",
        "    \"\"\"\n",
        "\n",
        "    # If Series is passed, convert it to DataFrame\n",
        "    if isinstance(data, pd.Series):\n",
        "        data = data.to_frame()\n",
        "\n",
        "    # Display dataset dimensions\n",
        "    print(f'Dataset size: {data.shape}')\n",
        "    print('-' * 100)\n",
        "\n",
        "    # Get column names\n",
        "    columns = data.columns\n",
        "\n",
        "    # Loop through all columns and display statistics\n",
        "    for column in columns:\n",
        "        # Display unique values\n",
        "        print(f'Unique values for feature: {column}')\n",
        "        print(data[column].unique())\n",
        "\n",
        "        # Count unique values\n",
        "        print(f'Number of unique values: {data[column].nunique()}')\n",
        "\n",
        "        # Count total values (non-null)\n",
        "        print(f'Total number of values: {data[column].count()}')\n",
        "\n",
        "        # Count missing values\n",
        "        print(f'Number of missing values in column {column}: {data[column].isna().sum()}')\n",
        "        print('-' * 100)"
      ],
      "metadata": {
        "id": "ShPUNm_lLK5X"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nan_counts(df):\n",
        "    \"\"\"\n",
        "    Analyzes missing values in the dataset and identifies rows with excessive NaN values.\n",
        "\n",
        "    The function calculates the number of missing values in each row and identifies\n",
        "    rows where more than half of the columns contain NaN values. This helps in\n",
        "    detecting severely incomplete records that may need to be removed or imputed.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        Input DataFrame for missing value analysis.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    None\n",
        "        The function prints results to the console without returning any value.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate number of missing values in each row\n",
        "    nan_counts = df.isna().sum(axis=1)\n",
        "\n",
        "    # Identify rows where more than half of the columns contain NaN values\n",
        "    half_nan_rows = nan_counts > len(df.columns) / 2\n",
        "\n",
        "    # Count rows with more than half NaN values\n",
        "    num_half_nan_rows = half_nan_rows.sum()\n",
        "\n",
        "    # Display results\n",
        "    print(f\"Total number of rows with more than half NaN values: {num_half_nan_rows}\")"
      ],
      "metadata": {
        "id": "0u68BJfRLnbG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analysis_num(data):\n",
        "    \"\"\"\n",
        "    Performs exploratory data analysis (EDA) for numerical data with visualizations.\n",
        "\n",
        "    The function generates three complementary plots (histogram, boxplot, and violin plot)\n",
        "    along with descriptive statistics to provide comprehensive insights into the\n",
        "    distribution and characteristics of numerical data.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : pandas.Series or array-like\n",
        "        Numerical data for analysis. Missing values will be automatically excluded.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    None\n",
        "        Displays plots and prints statistical summary to the console.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create subplots: histogram, boxplot, and violin plot\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "    # Plot 1: Histogram with mean and median lines\n",
        "    axes[0].hist(data.dropna(), bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "    axes[0].set_title('Distribution Histogram')\n",
        "    axes[0].set_xlabel('Values')\n",
        "    axes[0].set_ylabel('Frequency')\n",
        "\n",
        "    # Add vertical lines for median and mean\n",
        "    axes[0].axvline(data.median(), color='blue', linestyle='dashed', linewidth=2, label='Median')\n",
        "    axes[0].axvline(data.mean(), color='red', linestyle='solid', linewidth=2, label='Mean')\n",
        "    axes[0].legend(prop={'size': 8})\n",
        "\n",
        "    # Plot 2: Boxplot for outlier detection\n",
        "    axes[1].set_title('Box Plot')\n",
        "    box_plot = axes[1].boxplot(data.dropna(), patch_artist=True)\n",
        "    # Customize boxplot colors\n",
        "    box_plot['boxes'][0].set_facecolor('lightgreen')\n",
        "    axes[1].set_ylabel('Values')\n",
        "\n",
        "    # Plot 3: Violin plot showing distribution density\n",
        "    axes[2].set_title('Violin Plot')\n",
        "    violin_plot = axes[2].violinplot([data.dropna()], vert=False, widths=0.75,\n",
        "                                   showmeans=True, showmedians=True,\n",
        "                                   showextrema=True)\n",
        "    # Customize violin plot colors\n",
        "    violin_plot['bodies'][0].set_facecolor('lightcoral')\n",
        "    violin_plot['bodies'][0].set_alpha(0.7)\n",
        "    axes[2].set_xlabel('Values')\n",
        "\n",
        "    # Adjust layout and display plots\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Display descriptive statistics\n",
        "    print('\\n' + '='*60)\n",
        "    print('DESCRIPTIVE STATISTICS SUMMARY')\n",
        "    print('='*60)\n",
        "    print(data.describe())\n",
        "\n",
        "    # Calculate and display mode\n",
        "    mode_values = data.mode()\n",
        "    if not mode_values.empty:\n",
        "        print(f'\\nMode(s) in the dataset: {mode_values.values}')\n",
        "    else:\n",
        "        print('\\nNo unique mode found in the dataset')\n",
        "\n",
        "    # Additional useful statistics\n",
        "    print(f'Skewness: {data.skew():.3f}')\n",
        "    print(f'Kurtosis: {data.kurtosis():.3f}')\n",
        "    print(f'Missing values: {data.isna().sum()}')\n",
        "    print(f'Total observations: {len(data)}')"
      ],
      "metadata": {
        "id": "6A4RoEsAMewp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analysis_cat_pie(data, labels=None):\n",
        "    \"\"\"\n",
        "    Performs exploratory analysis of categorical data using a pie chart visualization.\n",
        "\n",
        "    The function calculates the frequency distribution of categorical values and displays\n",
        "    it as a pie chart with percentage labels. Optionally allows mapping of original values\n",
        "    to more descriptive labels.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : pandas.Series or array-like\n",
        "        Categorical data for analysis. Can contain text labels or numeric codes.\n",
        "    labels : dict or None, optional, default=None\n",
        "        Dictionary for mapping original values to descriptive labels.\n",
        "        If provided, keys should match original values and values will be used as labels.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    None\n",
        "        Displays a pie chart visualization.\n",
        "    \"\"\"\n",
        "\n",
        "    # Apply label mapping if provided\n",
        "    if labels is not None:\n",
        "        data = data.map(labels)\n",
        "\n",
        "    # Create pie chart with value counts and percentage labels\n",
        "    data.value_counts().plot(kind='pie', autopct='%.2f%%', figsize=(6, 6))\n",
        "    plt.title('Categorical Distribution Pie Chart', size=12)\n",
        "    plt.ylabel('')  # Remove y-axis label for better appearance\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "BuBZxarxNno9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analysis_cat_plot(data, ascending=True, figsize=(10, 6), color='skyblue'):\n",
        "    \"\"\"\n",
        "    Performs exploratory analysis of categorical data using a bar chart visualization.\n",
        "\n",
        "    The function calculates the frequency distribution of categorical values and displays\n",
        "    it as a horizontal bar chart sorted by frequency. This is useful for comparing\n",
        "    the prevalence of different categories in the dataset.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : pandas.Series or array-like\n",
        "        Categorical data for analysis. Can contain text labels or numeric codes.\n",
        "    ascending : bool, optional, default=True\n",
        "        If True, sorts categories by frequency in ascending order (smallest to largest).\n",
        "        If False, sorts in descending order (largest to smallest).\n",
        "    figsize : tuple, optional, default=(10, 6)\n",
        "        Figure size (width, height) in inches.\n",
        "    color : str, optional, default='skyblue'\n",
        "        Color of the bars in the chart.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    matplotlib.axes.Axes\n",
        "        The axes object containing the bar chart.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calculate value counts and create bar chart\n",
        "    value_counts = data.value_counts(ascending=ascending)\n",
        "\n",
        "    # Create the plot\n",
        "    ax = value_counts.plot(kind='barh', alpha=0.8, color=color, figsize=figsize)\n",
        "\n",
        "    # Customize the chart appearance\n",
        "    plt.title('Categorical Distribution Bar Chart', size=15, pad=20)\n",
        "    plt.xlabel('Frequency', size=12)\n",
        "    plt.ylabel('Categories', size=12)\n",
        "\n",
        "    # Add value labels on each bar\n",
        "    for i, v in enumerate(value_counts):\n",
        "        ax.text(v + 0.1, i, str(v), va='center', fontsize=10)\n",
        "\n",
        "    # Adjust layout and display\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print summary statistics\n",
        "    print(f\"Total categories: {len(value_counts)}\")\n",
        "    print(f\"Total observations: {len(data)}\")\n",
        "    print(f\"Most frequent category: '{value_counts.index[-1]}' ({value_counts.iloc[-1]} occurrences)\")\n",
        "    print(f\"Least frequent category: '{value_counts.index[0]}' ({value_counts.iloc[0]} occurrences)\")\n",
        "\n",
        "    return ax"
      ],
      "metadata": {
        "id": "IW3adLjzOBUU"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_lmplot(data, x, y, hue=None, figsize=(8, 6), title=None):\n",
        "    \"\"\"\n",
        "    Creates a linear regression plot (lmplot) to visualize the relationship between two variables.\n",
        "\n",
        "    The function generates a scatter plot with a linear regression line and confidence interval,\n",
        "    showing the correlation between two numerical variables. Optionally can stratify by a third\n",
        "    categorical variable.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : pandas.DataFrame\n",
        "        DataFrame containing the variables to plot.\n",
        "    x : str\n",
        "        Name of the column to use for the x-axis (independent variable).\n",
        "    y : str\n",
        "        Name of the column to use for the y-axis (dependent variable).\n",
        "    hue : str, optional, default=None\n",
        "        Name of categorical column to stratify the data by color.\n",
        "    figsize : tuple, optional, default=(8, 6)\n",
        "        Figure size (width, height) in inches.\n",
        "    title : str, optional, default=None\n",
        "        Custom title for the plot. If None, generates automatic title.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    seaborn.axisgrid.FacetGrid\n",
        "        The FacetGrid object containing the plot.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create the lmplot\n",
        "    g = sns.lmplot(x=x, y=y, data=data, hue=hue, height=figsize[1], aspect=figsize[0]/figsize[1])\n",
        "\n",
        "    # Set title and labels\n",
        "    if title is None:\n",
        "        title = f'Relationship between {y} and {x}'\n",
        "        if hue:\n",
        "            title += f' stratified by {hue}'\n",
        "\n",
        "    g.fig.suptitle(title, size=12, y=1.02)\n",
        "    g.set_xlabels(f'{x} variable', size=10)\n",
        "    g.set_ylabels(f'{y} variable', size=10)\n",
        "\n",
        "    # Adjust layout and display\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate and display correlation statistics\n",
        "    correlation = data[x].corr(data[y])\n",
        "    print(f\"Correlation coefficient between {x} and {y}: {correlation:.3f}\")\n",
        "\n",
        "    if correlation > 0.7:\n",
        "        strength = \"strong positive\"\n",
        "    elif correlation > 0.3:\n",
        "        strength = \"moderate positive\"\n",
        "    elif correlation > -0.3:\n",
        "        strength = \"weak\"\n",
        "    elif correlation > -0.7:\n",
        "        strength = \"moderate negative\"\n",
        "    else:\n",
        "        strength = \"strong negative\"\n",
        "\n",
        "    print(f\"Correlation strength: {strength}\")\n",
        "    print(f\"Number of observations: {len(data)}\")\n",
        "\n",
        "    return g"
      ],
      "metadata": {
        "id": "aK2pvBJ0Oy3p"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def target_feature_boxplot(data, target_col, feature_col, figsize=(8, 6), title=None):\n",
        "    \"\"\"\n",
        "    Analyzes the distribution of a numerical feature across different target groups using box plots.\n",
        "\n",
        "    The function creates a box plot to compare the distribution of a numerical feature\n",
        "    between different categories of a target variable. This is useful for understanding\n",
        "    how a feature differs between groups (e.g., healthy vs diseased patients).\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : pandas.DataFrame\n",
        "        DataFrame containing the target and feature columns.\n",
        "    target_col : str\n",
        "        Name of the column containing the target variable (categorical).\n",
        "    feature_col : str\n",
        "        Name of the column containing the numerical feature to analyze.\n",
        "    figsize : tuple, optional, default=(8, 6)\n",
        "        Figure size (width, height) in inches.\n",
        "    title : str, optional, default=None\n",
        "        Custom title for the plot. If None, generates automatic title.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    matplotlib.axes.Axes\n",
        "        The axes object containing the box plot.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create figure and box plot\n",
        "    plt.figure(figsize=figsize)\n",
        "    ax = sns.boxplot(x=target_col, y=feature_col, data=data, palette='Set2')\n",
        "\n",
        "    # Set title and labels\n",
        "    if title is None:\n",
        "        title = f'Distribution of {feature_col} by Target Groups'\n",
        "\n",
        "    plt.title(title, size=14, pad=20, fontweight='bold')\n",
        "    plt.xlabel('Target Group', size=12)\n",
        "    plt.ylabel(feature_col, size=12)\n",
        "\n",
        "    # Customize x-axis labels if binary target (0/1)\n",
        "    if set(data[target_col].unique()) == {0, 1}:\n",
        "        ax.set_xticklabels(['Without Disease (0)', 'With Disease (1)'])\n",
        "\n",
        "    # Add grid for better readability\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate and display group statistics\n",
        "    print('=' * 60)\n",
        "    print(f'GROUP COMPARISON: {feature_col} by {target_col}')\n",
        "    print('=' * 60)\n",
        "\n",
        "    groups = data.groupby(target_col)[feature_col]\n",
        "\n",
        "    for group_name, group_data in groups:\n",
        "        print(f\"\\nGroup {group_name} (n={len(group_data)}):\")\n",
        "        print(f\"  Mean: {group_data.mean():.2f}\")\n",
        "        print(f\"  Median: {group_data.median():.2f}\")\n",
        "        print(f\"  Std: {group_data.std():.2f}\")\n",
        "        print(f\"  Min: {group_data.min():.2f}\")\n",
        "        print(f\"  Max: {group_data.max():.2f}\")\n",
        "\n",
        "    # Perform basic statistical test (if exactly 2 groups)\n",
        "    if len(groups) == 2:\n",
        "        from scipy import stats\n",
        "        group1, group2 = list(groups)\n",
        "        _, p_value = stats.ttest_ind(group1[1].dropna(), group2[1].dropna())\n",
        "        print(f\"\\nT-test p-value: {p_value:.4f}\")\n",
        "        if p_value < 0.05:\n",
        "            print(\"Significant difference between groups (p < 0.05)\")\n",
        "        else:\n",
        "            print(\"No significant difference between groups (p >= 0.05)\")\n",
        "\n",
        "    return ax"
      ],
      "metadata": {
        "id": "j3WNpY2QPMUf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def target_group_categorical(data, target_col, feature_col, figsize=(10, 6), title=None,\n",
        "                           normalize=True, palette='Set2'):\n",
        "    \"\"\"\n",
        "    Analyzes the distribution of a categorical feature across target groups using stacked bar plots.\n",
        "\n",
        "    The function creates a stacked bar plot to compare the distribution of a categorical feature\n",
        "    between different categories of a target variable. This is useful for understanding\n",
        "    how categorical variables are distributed across target groups (e.g., disease vs no disease).\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : pandas.DataFrame\n",
        "        DataFrame containing the target and feature columns.\n",
        "    target_col : str\n",
        "        Name of the column containing the target variable (categorical).\n",
        "    feature_col : str\n",
        "        Name of the column containing the categorical feature to analyze.\n",
        "    figsize : tuple, optional, default=(10, 6)\n",
        "        Figure size (width, height) in inches.\n",
        "    title : str, optional, default=None\n",
        "        Custom title for the plot. If None, generates automatic title.\n",
        "    normalize : bool, optional, default=True\n",
        "        If True, shows percentages. If False, shows absolute counts.\n",
        "    palette : str or list, optional, default='Set2'\n",
        "        Color palette for the plot.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    matplotlib.axes.Axes\n",
        "        The axes object containing the bar plot.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create cross-tabulation\n",
        "    cross_tab = pd.crosstab(data[target_col], data[feature_col],\n",
        "                           normalize='index' if normalize else False)\n",
        "\n",
        "    # Create stacked bar plot\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    if normalize:\n",
        "        cross_tab.plot(kind='bar', stacked=True, ax=ax,\n",
        "                      color=palette, edgecolor='black')\n",
        "        ax.set_ylabel('Percentage', size=12)\n",
        "        # Add percentage labels on bars\n",
        "        for container in ax.containers:\n",
        "            ax.bar_label(container, labels=[f'{v.get_height()*100:.1f}%'\n",
        "                                          if v.get_height() > 0.05 else ''\n",
        "                                          for v in container],\n",
        "                        size=9, padding=2)\n",
        "    else:\n",
        "        cross_tab.plot(kind='bar', stacked=True, ax=ax,\n",
        "                      color=palette, edgecolor='black')\n",
        "        ax.set_ylabel('Count', size=12)\n",
        "        # Add count labels on bars\n",
        "        for container in ax.containers:\n",
        "            ax.bar_label(container, size=9, padding=2)\n",
        "\n",
        "    # Customize title and labels\n",
        "    if title is None:\n",
        "        title = f'Distribution of {feature_col} across {target_col} Groups'\n",
        "        if normalize:\n",
        "            title += ' (Percentages)'\n",
        "        else:\n",
        "            title += ' (Counts)'\n",
        "\n",
        "    plt.title(title, size=14, pad=20, fontweight='bold')\n",
        "    plt.xlabel(target_col, size=12)\n",
        "\n",
        "    # Customize x-axis labels for binary target\n",
        "    if set(data[target_col].unique()) == {0, 1}:\n",
        "        ax.set_xticklabels(['Without Disease (0)', 'With Disease (1)'], rotation=0)\n",
        "    else:\n",
        "        ax.set_xticklabels(ax.get_xticklabels(), rotation=0)\n",
        "\n",
        "    # Improve legend\n",
        "    plt.legend(title=feature_col, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Display statistical summary\n",
        "    print('=' * 70)\n",
        "    print(f'CATEGORICAL DISTRIBUTION ANALYSIS: {feature_col} by {target_col}')\n",
        "    print('=' * 70)\n",
        "\n",
        "    # Calculate and display summary statistics\n",
        "    contingency_table = pd.crosstab(data[target_col], data[feature_col])\n",
        "    print(\"\\nContingency Table (Counts):\")\n",
        "    print(contingency_table)\n",
        "\n",
        "    print(f\"\\nSummary Statistics:\")\n",
        "    print(f\"Total observations: {len(data)}\")\n",
        "    print(f\"Target variable: {target_col} ({len(data[target_col].unique())} categories)\")\n",
        "    print(f\"Feature variable: {feature_col} ({len(data[feature_col].unique())} categories)\")\n",
        "    print(f\"Missing values - Target: {data[target_col].isna().sum()}, \"\n",
        "          f\"Feature: {data[feature_col].isna().sum()}\")\n",
        "\n",
        "    # Chi-square test for association\n",
        "    if len(contingency_table) > 1 and len(contingency_table.columns) > 1:\n",
        "        from scipy.stats import chi2_contingency\n",
        "        chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "        print(f\"\\nChi-square Test of Independence:\")\n",
        "        print(f\"  Chi-square statistic: {chi2:.3f}\")\n",
        "        print(f\"  P-value: {p_value:.4f}\")\n",
        "        print(f\"  Degrees of freedom: {dof}\")\n",
        "        if p_value < 0.05:\n",
        "            print(\"  Significant association between variables (p < 0.05)\")\n",
        "        else:\n",
        "            print(\"  No significant association (p >= 0.05)\")\n",
        "\n",
        "    return ax"
      ],
      "metadata": {
        "id": "GCAQ9izfQLt2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_grouped_data(dataframe, group_cols, rename_dict=None, figsize=(10, 6),\n",
        "                     plot_type='stacked', colormap='tab10', title=None):\n",
        "    \"\"\"\n",
        "    Creates a histogram/bar chart to compare feature distribution across two categorical groups.\n",
        "\n",
        "    The function groups data by two categorical variables and creates a bar chart visualization\n",
        "    to compare their distributions. Useful for analyzing relationships between two categorical features.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    dataframe : pandas.DataFrame\n",
        "        Input DataFrame containing the categorical variables to analyze.\n",
        "    group_cols : list of str\n",
        "        List of two column names for grouping [primary_group, secondary_group].\n",
        "    rename_dict : dict, optional, default=None\n",
        "        Dictionary for renaming columns and indices. Format:\n",
        "        {'columns': {old: new}, 'index': {old: new}}\n",
        "    figsize : tuple, optional, default=(10, 6)\n",
        "        Figure size (width, height) in inches.\n",
        "    plot_type : str, optional, default='stacked'\n",
        "        Type of bar chart: 'stacked', 'grouped', or 'percent'.\n",
        "    colormap : str, optional, default='tab10'\n",
        "        Matplotlib colormap for the plot.\n",
        "    title : str, optional, default=None\n",
        "        Custom title for the plot.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        The grouped data used for plotting.\n",
        "    matplotlib.axes.Axes\n",
        "        The axes object containing the plot.\n",
        "    \"\"\"\n",
        "\n",
        "    # Group data by the specified columns\n",
        "    grouped_data = dataframe.groupby(group_cols).size().unstack(fill_value=0)\n",
        "\n",
        "    # Apply renaming if provided\n",
        "    if rename_dict is not None:\n",
        "        grouped_data.rename(columns=rename_dict.get('columns'),\n",
        "                          index=rename_dict.get('index'), inplace=True)\n",
        "\n",
        "    # Create the plot\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    if plot_type == 'stacked':\n",
        "        ax = grouped_data.plot(kind='bar', stacked=True, colormap=colormap, alpha=0.8)\n",
        "    elif plot_type == 'grouped':\n",
        "        ax = grouped_data.plot(kind='bar', colormap=colormap, alpha=0.8)\n",
        "    elif plot_type == 'percent':\n",
        "        percent_data = grouped_data.div(grouped_data.sum(axis=1), axis=0) * 100\n",
        "        ax = percent_data.plot(kind='bar', stacked=True, colormap=colormap, alpha=0.8)\n",
        "    else:\n",
        "        ax = grouped_data.plot(kind='bar', stacked=True, colormap=colormap, alpha=0.8)\n",
        "\n",
        "    # Customize labels and title\n",
        "    if title is None:\n",
        "        title = f'Distribution of {group_cols[1]} by {group_cols[0]}'\n",
        "\n",
        "    plt.xlabel(f'{group_cols[0]} Category', size=12)\n",
        "    plt.ylabel('Number of Records', size=12)\n",
        "    plt.title(title, size=14, pad=20, fontweight='bold')\n",
        "\n",
        "    # Customize legend\n",
        "    plt.legend(title=group_cols[1], bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "    # Add grid and styling\n",
        "    plt.grid(axis='y', alpha=0.3, linestyle='--')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Display data summary\n",
        "    print('=' * 60)\n",
        "    print('GROUPED DATA SUMMARY')\n",
        "    print('=' * 60)\n",
        "    print(f\"Primary grouping variable: {group_cols[0]}\")\n",
        "    print(f\"Secondary grouping variable: {group_cols[1]}\")\n",
        "    print(f\"Total records: {len(dataframe)}\")\n",
        "    print(f\"\\nGrouped data table:\")\n",
        "    print(grouped_data)\n",
        "\n",
        "    return grouped_data, ax"
      ],
      "metadata": {
        "id": "-MwBfYziRNIz"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, X_test, y_test, figsize=(12, 5)):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation of model performance on test data.\n",
        "\n",
        "    The function calculates ROC-AUC score, plots ROC curve, and displays confusion matrix\n",
        "    to provide a complete assessment of binary classification model performance.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : sklearn classifier\n",
        "        Trained classification model with predict_proba method.\n",
        "    X_test : array-like of shape (n_samples, n_features)\n",
        "        Test features for evaluation.\n",
        "    y_test : array-like of shape (n_samples,)\n",
        "        True labels for test data.\n",
        "    figsize : tuple, optional, default=(12, 5)\n",
        "        Figure size for the plots.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary containing evaluation metrics:\n",
        "        - 'roc_auc': ROC-AUC score\n",
        "        - 'fpr': False Positive Rates\n",
        "        - 'tpr': True Positive Rates\n",
        "        - 'confusion_matrix': Confusion matrix array\n",
        "        - 'predictions': Model predictions\n",
        "    \"\"\"\n",
        "\n",
        "    print('ROC-AUC Evaluation on Test Data')\n",
        "    print('=' * 50)\n",
        "\n",
        "    # Get predicted probabilities for positive class\n",
        "    probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "    # Calculate ROC curve metrics\n",
        "    fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    print(f'ROC-AUC Score on Test Data: {roc_auc:.4f}')\n",
        "\n",
        "    # Create subplots for ROC curve and confusion matrix\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
        "\n",
        "    # Plot ROC Curve\n",
        "    lw = 2\n",
        "    ax1.plot(fpr, tpr, color='darkorange', lw=lw,\n",
        "             label=f'ROC curve (area = {roc_auc:.3f})')\n",
        "    ax1.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--',\n",
        "             label='Random classifier')\n",
        "    ax1.set_xlim([0.0, 1.0])\n",
        "    ax1.set_ylim([0.0, 1.05])\n",
        "    ax1.set_xlabel('False Positive Rate (FPR)')\n",
        "    ax1.set_ylabel('True Positive Rate (TPR)')\n",
        "    ax1.set_title('Receiver Operating Characteristic (ROC) Curve')\n",
        "    ax1.legend(loc=\"lower right\")\n",
        "    ax1.grid(alpha=0.3)\n",
        "\n",
        "    # Generate predictions and confusion matrix\n",
        "    predictions = model.predict(X_test)\n",
        "    cm = confusion_matrix(y_test, predictions)\n",
        "\n",
        "    # Plot Confusion Matrix\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax2)\n",
        "    ax2.set_xlabel('Predicted Labels')\n",
        "    ax2.set_ylabel('True Labels')\n",
        "    ax2.set_title('Confusion Matrix')\n",
        "\n",
        "    # Add class labels for binary classification\n",
        "    if cm.shape == (2, 2):\n",
        "        ax2.set_xticklabels(['Negative (0)', 'Positive (1)'])\n",
        "        ax2.set_yticklabels(['Negative (0)', 'Positive (1)'])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Calculate additional metrics\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    precision = precision_score(y_test, predictions)\n",
        "    recall = recall_score(y_test, predictions)\n",
        "    f1 = f1_score(y_test, predictions)\n",
        "\n",
        "    # Display comprehensive metrics\n",
        "    print('\\nDetailed Performance Metrics:')\n",
        "    print('-' * 40)\n",
        "    print(f'Accuracy:  {accuracy:.4f}')\n",
        "    print(f'Precision: {precision:.4f}')\n",
        "    print(f'Recall:    {recall:.4f}')\n",
        "    print(f'F1-Score:  {f1:.4f}')\n",
        "    print(f'ROC-AUC:   {roc_auc:.4f}')\n",
        "\n",
        "    # Interpretation of ROC-AUC\n",
        "    print('\\nROC-AUC Interpretation:')\n",
        "    print('-' * 40)\n",
        "    if roc_auc >= 0.9:\n",
        "        interpretation = \"Excellent discrimination\"\n",
        "    elif roc_auc >= 0.8:\n",
        "        interpretation = \"Good discrimination\"\n",
        "    elif roc_auc >= 0.7:\n",
        "        interpretation = \"Fair discrimination\"\n",
        "    elif roc_auc >= 0.6:\n",
        "        interpretation = \"Poor discrimination\"\n",
        "    else:\n",
        "        interpretation = \"No discrimination\"\n",
        "    print(f'{interpretation} (AUC = {roc_auc:.3f})')\n",
        "\n",
        "    # Confusion matrix breakdown\n",
        "    if cm.shape == (2, 2):\n",
        "        tn, fp, fn, tp = cm.ravel()\n",
        "        print('\\nConfusion Matrix Breakdown:')\n",
        "        print('-' * 40)\n",
        "        print(f'True Negatives:  {tn}')\n",
        "        print(f'False Positives: {fp}')\n",
        "        print(f'False Negatives: {fn}')\n",
        "        print(f'True Positives:  {tp}')\n",
        "        print(f'Sensitivity (Recall): {tp/(tp+fn):.3f}')\n",
        "        print(f'Specificity: {tn/(tn+fp):.3f}')\n",
        "\n",
        "    return {\n",
        "        'roc_auc': roc_auc,\n",
        "        'fpr': fpr,\n",
        "        'tpr': tpr,\n",
        "        'thresholds': thresholds,\n",
        "        'confusion_matrix': cm,\n",
        "        'predictions': predictions,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1\n",
        "    }"
      ],
      "metadata": {
        "id": "m7AuvxJbR7tm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def regression_metrics(y_train, y_train_predict, y_test, y_test_predict):\n",
        "    \"\"\"\n",
        "    Calculates regression model quality metrics for training and test datasets.\n",
        "\n",
        "    Parameters:\n",
        "    ----------\n",
        "    y_train : array-like\n",
        "        True target values for the training set\n",
        "    y_train_predict : array-like\n",
        "        Predicted values for the training set\n",
        "    y_test : array-like\n",
        "        True target values for the test set\n",
        "    y_test_predict : array-like\n",
        "        Predicted values for the test set\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    list[float, float]\n",
        "        List of two RMSE (Root Mean Squared Error) values rounded to 3 decimal places:\n",
        "        - First element: RMSE on training data\n",
        "        - Second element: RMSE on test data\n",
        "    \"\"\"\n",
        "\n",
        "    rmse_train = sqrt(metrics.mean_squared_error(y_train, y_train_predict))\n",
        "    rmse_test = sqrt(metrics.mean_squared_error(y_test, y_test_predict))\n",
        "\n",
        "    return [round(rmse_train, 3), round(rmse_test, 3)]"
      ],
      "metadata": {
        "id": "UIyK3SYONeAL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I. Data loading and exploration\n",
        "_______________________________________________\n",
        "1) Downloading files from Kaggle"
      ],
      "metadata": {
        "id": "5muMUJHjSvZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install kaggle"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPU9vNn6S45Y",
        "outputId": "4a9b4aae-a64d-4772-c095-0eab66269844"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.8.3)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.3)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c tech-weekend-data-science-hackathon"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpiBFdHZXjbS",
        "outputId": "e3308c0e-b90f-44aa-ad11-f0d64145075b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 4, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/__init__.py\", line 6, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/kaggle/api/kaggle_api_extended.py\", line 434, in authenticate\n",
            "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.config/kaggle. Or use the environment method. See setup instructions at https://github.com/Kaggle/kaggle-api/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train.head(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "0ojgu1C8Z_2V",
        "outputId": "3f26ffbf-ebe5-4306-e9c6-9417a9bd375c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1281434720.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train' is not defined"
          ]
        }
      ]
    }
  ]
}